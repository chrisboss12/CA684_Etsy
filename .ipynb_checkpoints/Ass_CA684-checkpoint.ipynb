{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddabe719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Bidirectional, Dense\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.layers import MaxPooling1D, Dropout, Activation, Conv1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MultiLabelBinarizer, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn.impute\n",
    "from sklearn.impute import SimpleImputer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "PATH = 'C:/Users/chris/CA684_ML_Assignment/data'\n",
    "parquet_filenames_train = !ls {PATH}/parquet/train\n",
    "tfrecords_filenames_train = !ls {PATH}/tfrecords/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42545ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to your data directory\n",
    "pq_path = 'C:/Users/chris/CA684_ML_Assignment/data/parquet/train'\n",
    "\n",
    "# read all Parquet files in the directory into a single PyArrow table\n",
    "table = pq.read_table(pq_path)\n",
    "\n",
    "# convert the table to a Pandas DataFrame\n",
    "df_train = table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c213ba09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>type</th>\n",
       "      <th>room</th>\n",
       "      <th>craft_type</th>\n",
       "      <th>recipient</th>\n",
       "      <th>material</th>\n",
       "      <th>occasion</th>\n",
       "      <th>...</th>\n",
       "      <th>art_subject</th>\n",
       "      <th>style</th>\n",
       "      <th>shape</th>\n",
       "      <th>pattern</th>\n",
       "      <th>bottom_category_id</th>\n",
       "      <th>bottom_category_text</th>\n",
       "      <th>top_category_id</th>\n",
       "      <th>top_category_text</th>\n",
       "      <th>color_id</th>\n",
       "      <th>color_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>543497833</td>\n",
       "      <td>Full-face custom motorcycle helmet ,Motorcycle...</td>\n",
       "      <td>Helmetartthai from Thailand ( Since 2016) \\n\\n...</td>\n",
       "      <td>predator helmet,motorcycle helmet,helmet,handm...</td>\n",
       "      <td>physical</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2804</td>\n",
       "      <td>accessories.hats_and_caps.helmets.sports_helme...</td>\n",
       "      <td>0</td>\n",
       "      <td>accessories</td>\n",
       "      <td>12</td>\n",
       "      <td>purple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718680498</td>\n",
       "      <td>Full-face predator motorcycle  helmet ,Custom ...</td>\n",
       "      <td>Helmetartthai from Thailand ( Since 2016) \\n\\n...</td>\n",
       "      <td>Predator helmet,Custom helmet,Handmade helme,P...</td>\n",
       "      <td>physical</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2804</td>\n",
       "      <td>accessories.hats_and_caps.helmets.sports_helme...</td>\n",
       "      <td>0</td>\n",
       "      <td>accessories</td>\n",
       "      <td>1</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>718823736</td>\n",
       "      <td>Full-face custom motorcycle helmet ,Motorcycle...</td>\n",
       "      <td>Helmetartthai from Thailand ( Since 2016) \\n\\n...</td>\n",
       "      <td>Predator helmet,Custom helmet,Handmade helmet,...</td>\n",
       "      <td>physical</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2804</td>\n",
       "      <td>accessories.hats_and_caps.helmets.sports_helme...</td>\n",
       "      <td>0</td>\n",
       "      <td>accessories</td>\n",
       "      <td>2</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>608840803</td>\n",
       "      <td>Full-face custom motorcycle helmet ,Motorcycle...</td>\n",
       "      <td>Helmetartthai from Thailand ( Since 2016) \\n\\n...</td>\n",
       "      <td>Predator helmet,Custom helmet,Handmade helmet,...</td>\n",
       "      <td>physical</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2804</td>\n",
       "      <td>accessories.hats_and_caps.helmets.sports_helme...</td>\n",
       "      <td>0</td>\n",
       "      <td>accessories</td>\n",
       "      <td>4</td>\n",
       "      <td>brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>804070543</td>\n",
       "      <td>Full-face predator motorcycle  helmet ,Custom ...</td>\n",
       "      <td>Helmetartthai from Thailand ( Since 2016) \\n\\n...</td>\n",
       "      <td>Custom helmet,Handmade helmet,Predator helmet,...</td>\n",
       "      <td>physical</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2804</td>\n",
       "      <td>accessories.hats_and_caps.helmets.sports_helme...</td>\n",
       "      <td>0</td>\n",
       "      <td>accessories</td>\n",
       "      <td>1</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245480</th>\n",
       "      <td>1330991677</td>\n",
       "      <td>Vintage - 1980&amp;#39;s - APRI NIGHTS - Black Alu...</td>\n",
       "      <td>This is a charming Vintage - 1980&amp;#39;s - APRI...</td>\n",
       "      <td>Black Mesh Beading,Apri Nights,Evening Bag,Cro...</td>\n",
       "      <td>physical</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>prom</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>157</td>\n",
       "      <td>bags_and_purses.handbags.clutches_and_evening_...</td>\n",
       "      <td>2</td>\n",
       "      <td>bags_and_purses</td>\n",
       "      <td>1</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245481</th>\n",
       "      <td>927998323</td>\n",
       "      <td>Gold Eloquent Envelope Clutch bag</td>\n",
       "      <td>Looking for a classy posh bag to hit up a hot ...</td>\n",
       "      <td>clutch,gold,soft,envelope,disco</td>\n",
       "      <td>physical</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>wedding</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>157</td>\n",
       "      <td>bags_and_purses.handbags.clutches_and_evening_...</td>\n",
       "      <td>2</td>\n",
       "      <td>bags_and_purses</td>\n",
       "      <td>7</td>\n",
       "      <td>gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245482</th>\n",
       "      <td>1393052238</td>\n",
       "      <td>Personalized pouch, toiletry bag, makeup bag</td>\n",
       "      <td>Personalized items are not returned or exchang...</td>\n",
       "      <td>None</td>\n",
       "      <td>physical</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>bachelorette party</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>157</td>\n",
       "      <td>bags_and_purses.handbags.clutches_and_evening_...</td>\n",
       "      <td>2</td>\n",
       "      <td>bags_and_purses</td>\n",
       "      <td>17</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245483</th>\n",
       "      <td>775199837</td>\n",
       "      <td>Floral handbag, Canvas bag, Bridesmaid bag, gu...</td>\n",
       "      <td>Handbag of flowers and jute ideal for day part...</td>\n",
       "      <td>Flower handbag,Canvas bag,Bridesmaid bag,guest...</td>\n",
       "      <td>physical</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>bridal shower</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>157</td>\n",
       "      <td>bags_and_purses.handbags.clutches_and_evening_...</td>\n",
       "      <td>2</td>\n",
       "      <td>bags_and_purses</td>\n",
       "      <td>0</td>\n",
       "      <td>beige</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245484</th>\n",
       "      <td>1136519781</td>\n",
       "      <td>Modern Marrakesh Bohemian Seed Beaded Clutch B...</td>\n",
       "      <td>Modern Marrakesh Bohemian Seed Beaded Clutch B...</td>\n",
       "      <td>Beaded Clutch,Seed Bead Bags,Beaded Bags,Bead ...</td>\n",
       "      <td>physical</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>157</td>\n",
       "      <td>bags_and_purses.handbags.clutches_and_evening_...</td>\n",
       "      <td>2</td>\n",
       "      <td>bags_and_purses</td>\n",
       "      <td>13</td>\n",
       "      <td>rainbow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245485 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        product_id                                              title  \\\n",
       "0        543497833  Full-face custom motorcycle helmet ,Motorcycle...   \n",
       "1        718680498  Full-face predator motorcycle  helmet ,Custom ...   \n",
       "2        718823736  Full-face custom motorcycle helmet ,Motorcycle...   \n",
       "3        608840803  Full-face custom motorcycle helmet ,Motorcycle...   \n",
       "4        804070543  Full-face predator motorcycle  helmet ,Custom ...   \n",
       "...            ...                                                ...   \n",
       "245480  1330991677  Vintage - 1980&#39;s - APRI NIGHTS - Black Alu...   \n",
       "245481   927998323                  Gold Eloquent Envelope Clutch bag   \n",
       "245482  1393052238       Personalized pouch, toiletry bag, makeup bag   \n",
       "245483   775199837  Floral handbag, Canvas bag, Bridesmaid bag, gu...   \n",
       "245484  1136519781  Modern Marrakesh Bohemian Seed Beaded Clutch B...   \n",
       "\n",
       "                                              description  \\\n",
       "0       Helmetartthai from Thailand ( Since 2016) \\n\\n...   \n",
       "1       Helmetartthai from Thailand ( Since 2016) \\n\\n...   \n",
       "2       Helmetartthai from Thailand ( Since 2016) \\n\\n...   \n",
       "3       Helmetartthai from Thailand ( Since 2016) \\n\\n...   \n",
       "4       Helmetartthai from Thailand ( Since 2016) \\n\\n...   \n",
       "...                                                   ...   \n",
       "245480  This is a charming Vintage - 1980&#39;s - APRI...   \n",
       "245481  Looking for a classy posh bag to hit up a hot ...   \n",
       "245482  Personalized items are not returned or exchang...   \n",
       "245483  Handbag of flowers and jute ideal for day part...   \n",
       "245484  Modern Marrakesh Bohemian Seed Beaded Clutch B...   \n",
       "\n",
       "                                                     tags      type  room  \\\n",
       "0       predator helmet,motorcycle helmet,helmet,handm...  physical  None   \n",
       "1       Predator helmet,Custom helmet,Handmade helme,P...  physical  None   \n",
       "2       Predator helmet,Custom helmet,Handmade helmet,...  physical  None   \n",
       "3       Predator helmet,Custom helmet,Handmade helmet,...  physical  None   \n",
       "4       Custom helmet,Handmade helmet,Predator helmet,...  physical  None   \n",
       "...                                                   ...       ...   ...   \n",
       "245480  Black Mesh Beading,Apri Nights,Evening Bag,Cro...  physical  None   \n",
       "245481                    clutch,gold,soft,envelope,disco  physical  None   \n",
       "245482                                               None  physical  None   \n",
       "245483  Flower handbag,Canvas bag,Bridesmaid bag,guest...  physical  None   \n",
       "245484  Beaded Clutch,Seed Bead Bags,Beaded Bags,Bead ...  physical  None   \n",
       "\n",
       "       craft_type recipient material            occasion  ... art_subject  \\\n",
       "0            None      None     None                None  ...        None   \n",
       "1            None      None     None                None  ...        None   \n",
       "2            None      None     None                None  ...        None   \n",
       "3            None      None     None                None  ...        None   \n",
       "4            None      None     None                None  ...        None   \n",
       "...           ...       ...      ...                 ...  ...         ...   \n",
       "245480       None      None     None                prom  ...        None   \n",
       "245481       None      None     None             wedding  ...        None   \n",
       "245482       None      None     None  bachelorette party  ...        None   \n",
       "245483       None      None     None       bridal shower  ...        None   \n",
       "245484       None      None     None                None  ...        None   \n",
       "\n",
       "       style shape pattern bottom_category_id  \\\n",
       "0       None  None    None               2804   \n",
       "1       None  None    None               2804   \n",
       "2       None  None    None               2804   \n",
       "3       None  None    None               2804   \n",
       "4       None  None    None               2804   \n",
       "...      ...   ...     ...                ...   \n",
       "245480  None  None    None                157   \n",
       "245481  None  None    None                157   \n",
       "245482  None  None    None                157   \n",
       "245483  None  None    None                157   \n",
       "245484  None  None    None                157   \n",
       "\n",
       "                                     bottom_category_text top_category_id  \\\n",
       "0       accessories.hats_and_caps.helmets.sports_helme...               0   \n",
       "1       accessories.hats_and_caps.helmets.sports_helme...               0   \n",
       "2       accessories.hats_and_caps.helmets.sports_helme...               0   \n",
       "3       accessories.hats_and_caps.helmets.sports_helme...               0   \n",
       "4       accessories.hats_and_caps.helmets.sports_helme...               0   \n",
       "...                                                   ...             ...   \n",
       "245480  bags_and_purses.handbags.clutches_and_evening_...               2   \n",
       "245481  bags_and_purses.handbags.clutches_and_evening_...               2   \n",
       "245482  bags_and_purses.handbags.clutches_and_evening_...               2   \n",
       "245483  bags_and_purses.handbags.clutches_and_evening_...               2   \n",
       "245484  bags_and_purses.handbags.clutches_and_evening_...               2   \n",
       "\n",
       "        top_category_text color_id  color_text  \n",
       "0             accessories       12      purple  \n",
       "1             accessories        1       black  \n",
       "2             accessories        2        blue  \n",
       "3             accessories        4       brown  \n",
       "4             accessories        1       black  \n",
       "...                   ...      ...         ...  \n",
       "245480    bags_and_purses        1       black  \n",
       "245481    bags_and_purses        7        gold  \n",
       "245482    bags_and_purses       17       white  \n",
       "245483    bags_and_purses        0       beige  \n",
       "245484    bags_and_purses       13     rainbow  \n",
       "\n",
       "[245485 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79f8286d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Try to run on TPU\n",
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fe43af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying directories to save the trained model and checkpoints\n",
    "CURR_PATH = 'C:/Users/chris/CA684_ML_Assignment/'\n",
    "PATH_DATA = CURR_PATH\n",
    "PATH_MODELS = PATH_DATA + \"models/\"\n",
    "PATH_CHECKPOINTS = PATH_MODELS + \"checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00e8959c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Text Vectors\n",
    "MAX_FEATURES = 30000\n",
    "EMBED_DIM = 300\n",
    "MAXLEN = 400\n",
    "\n",
    "# Convolution\n",
    "KERNEL_SIZE = 5\n",
    "FILTERS = 64\n",
    "POOL_SIZE = 4\n",
    "\n",
    "# LSTM\n",
    "LSTM_OUTPUT_SIZE = 100\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b38fef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    " # Loading \"stopwords\" and \"SnowballStemmer\" based on German language to remove morphological affixes from German words\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c6b7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Function to perform cleaning of textual data\n",
    "def clean_text(text, for_embedding=False):\n",
    "    \"\"\"\n",
    "        - remove any html tags (< /br> often found)\n",
    "        - Keep only ASCII + European Chars and whitespace, no digits\n",
    "        - remove single letter chars\n",
    "        - convert all whitespaces (tabs etc.) to single wspace\n",
    "        if not for embedding (but e.g. tdf-idf):\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and stemm\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "    if for_embedding:\n",
    "        # Keep punctuation\n",
    "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "\n",
    "    word_tokens = word_tokenize(text)\n",
    "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_filtered = [\n",
    "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
    "        ]\n",
    "\n",
    "    text_clean = \" \".join(words_filtered)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed6d7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['combined_data'] = [str(df_train['title'][i]) + \" \" \n",
    "                                + str(df_train['description'][i]) + \" \" \n",
    "                                + str(df_train['tags'][i]) + \" \" \n",
    "                                + str(df_train['room'][i]) + \" \"\n",
    "                                + str(df_train['craft_type'][i]) + \" \"\n",
    "                                + str(df_train['recipient'][i]) + \" \"\n",
    "                                + str(df_train['material'][i]) + \" \"\n",
    "                                + str(df_train['occasion'][i]) + \" \"\n",
    "                                + str(df_train['holiday'][i]) + \" \"\n",
    "                                + str(df_train['art_subject'][i]) + \" \"\n",
    "                                + str(df_train['style'][i]) + \" \"\n",
    "                                + str(df_train['shape'][i]) + \" \"\n",
    "                                + str(df_train['pattern'][i]) + \" \"\n",
    "                                + str(df_train['top_category_text'][i]) + \" \"\n",
    "                                + str(df_train['bottom_category_text'][i]) + \" \"\n",
    "                                + str(df_train['color_text'][i])\n",
    "                                for i in range(len(df_train))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a74ab3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(df_train['combined_data'])\n",
    "sequences = tokenizer.texts_to_sequences(df_train['combined_data'])\n",
    "\n",
    "# Pad the sequences\n",
    "X = pad_sequences(sequences, maxlen=MAXLEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff2b0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, df_train[['color_id', 'top_category_id', 'bottom_category_id']], \n",
    "    test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71ccdd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 400, 300)          9000000   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 400, 300)          0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 396, 64)           96064     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 99, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               66000     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,162,165\n",
      "Trainable params: 9,162,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "\n",
    "# Define NN architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=MAX_FEATURES, output_dim=EMBED_DIM, input_length=MAXLEN))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(FILTERS, KERNEL_SIZE, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(pool_size=POOL_SIZE))\n",
    "model.add(LSTM(LSTM_OUTPUT_SIZE))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "065679e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=\"category_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "255cb5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 184, in __call__\n        self.build(y_pred)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 133, in build\n        self._losses = tf.nest.map_structure(self._get_loss_object, self._losses)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 272, in _get_loss_object\n        loss = losses_mod.get(loss)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\losses.py\", line 2369, in get\n        return deserialize(identifier)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\losses.py\", line 2324, in deserialize\n        return deserialize_keras_object(\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 709, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown loss function: category_crossentropy. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 184, in __call__\n        self.build(y_pred)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 133, in build\n        self._losses = tf.nest.map_structure(self._get_loss_object, self._losses)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 272, in _get_loss_object\n        loss = losses_mod.get(loss)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\losses.py\", line 2369, in get\n        return deserialize(identifier)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\losses.py\", line 2324, in deserialize\n        return deserialize_keras_object(\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 709, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown loss function: category_crossentropy. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "scores = model.evaluate(val_sequences, val_data[['color_id', 'top_category_id', 'bottom_category_id']])\n",
    "print('Validation loss:', scores[0])\n",
    "print('Validation accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19522a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
